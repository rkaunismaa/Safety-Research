{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Vectors: Accurate Repository Implementation\n",
    "\n",
    "This notebook provides an accurate implementation of **Persona Vectors** that matches the behavior of the `persona_vectors` repository exactly.\n",
    "\n",
    "**Model**: Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "## Key Implementation Details (Matching Repository)\n",
    "\n",
    "1. **Steering**: Only steers the **last token** during generation (not all response tokens)\n",
    "2. **Layer Indexing**: Uses `layer_idx = layer - 1` offset when creating hooks\n",
    "3. **Tokenization**: Uses `add_special_tokens=False` for hidden state extraction\n",
    "4. **Architecture-agnostic**: Supports multiple model architectures via `_locate_layer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional, Sequence, Union, Iterable\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen2.5-7B-Instruct\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ActivationSteerer - Exact Repository Implementation\n",
    "\n",
    "This is copied directly from `persona_vectors/activation_steer.py` with minor adaptations for notebook use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSteerer:\n",
    "    \"\"\"\n",
    "    Add (coeff * steering_vector) to a chosen transformer block's output.\n",
    "    \n",
    "    EXACT implementation from persona_vectors/activation_steer.py\n",
    "    \n",
    "    Key behaviors:\n",
    "    - 'all': Add steering to ALL tokens\n",
    "    - 'prompt': Add steering to all tokens EXCEPT during single-token generation\n",
    "    - 'response': Add steering ONLY to the LAST token (t[:, -1, :])\n",
    "    \"\"\"\n",
    "\n",
    "    _POSSIBLE_LAYER_ATTRS: Iterable[str] = (\n",
    "        \"transformer.h\",       # GPT-2/Neo, Bloom, etc.\n",
    "        \"encoder.layer\",       # BERT/RoBERTa\n",
    "        \"model.layers\",        # Llama/Mistral/Qwen\n",
    "        \"gpt_neox.layers\",     # GPT-NeoX\n",
    "        \"block\",               # Flan-T5\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        steering_vector: Union[torch.Tensor, Sequence[float]],\n",
    "        *,\n",
    "        coeff: float = 1.0,\n",
    "        layer_idx: int = -1,\n",
    "        positions: str = \"all\",\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.model, self.coeff, self.layer_idx = model, float(coeff), layer_idx\n",
    "        self.positions = positions.lower()\n",
    "        self.debug = debug\n",
    "        self._handle = None\n",
    "\n",
    "        # Build vector\n",
    "        p = next(model.parameters())\n",
    "        self.vector = torch.as_tensor(steering_vector, dtype=p.dtype, device=p.device)\n",
    "        if self.vector.ndim != 1:\n",
    "            raise ValueError(\"steering_vector must be 1-D\")\n",
    "        hidden = getattr(model.config, \"hidden_size\", None)\n",
    "        if hidden and self.vector.numel() != hidden:\n",
    "            raise ValueError(\n",
    "                f\"Vector length {self.vector.numel()} != model hidden_size {hidden}\"\n",
    "            )\n",
    "        # Validate positions\n",
    "        valid_positions = {\"all\", \"prompt\", \"response\"}\n",
    "        if self.positions not in valid_positions:\n",
    "            raise ValueError(\"positions must be 'all', 'prompt', 'response'\")\n",
    "\n",
    "    def _locate_layer(self):\n",
    "        \"\"\"Architecture-agnostic layer location.\"\"\"\n",
    "        for path in self._POSSIBLE_LAYER_ATTRS:\n",
    "            cur = self.model\n",
    "            for part in path.split(\".\"):\n",
    "                if hasattr(cur, part):\n",
    "                    cur = getattr(cur, part)\n",
    "                else:\n",
    "                    break\n",
    "            else:  # Found a full match\n",
    "                if not hasattr(cur, \"__getitem__\"):\n",
    "                    continue\n",
    "                if not (-len(cur) <= self.layer_idx < len(cur)):\n",
    "                    raise IndexError(f\"layer_idx {self.layer_idx} out of range for {len(cur)} layers\")\n",
    "                if self.debug:\n",
    "                    print(f\"[ActivationSteerer] hooking {path}[{self.layer_idx}]\")\n",
    "                return cur[self.layer_idx]\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Could not find layer list on the model. \"\n",
    "            \"Add the attribute name to _POSSIBLE_LAYER_ATTRS.\"\n",
    "        )\n",
    "\n",
    "    def _hook_fn(self, module, ins, out):\n",
    "        \"\"\"Hook function - CRITICAL: 'response' only steers LAST token!\"\"\"\n",
    "        steer = self.coeff * self.vector  # (hidden,)\n",
    "\n",
    "        def _add(t):\n",
    "            if self.positions == \"all\":\n",
    "                return t + steer.to(t.device)\n",
    "            elif self.positions == \"prompt\":\n",
    "                # Don't steer during single-token generation (autoregressive step)\n",
    "                if t.shape[1] == 1:\n",
    "                    return t\n",
    "                else:\n",
    "                    t2 = t.clone()\n",
    "                    t2 += steer.to(t.device)\n",
    "                    return t2\n",
    "            elif self.positions == \"response\":\n",
    "                # CRITICAL: Only steer the LAST token!\n",
    "                t2 = t.clone()\n",
    "                t2[:, -1, :] += steer.to(t.device)\n",
    "                return t2\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid positions: {self.positions}\")\n",
    "\n",
    "        # Handle tuple or tensor output\n",
    "        if torch.is_tensor(out):\n",
    "            new_out = _add(out)\n",
    "        elif isinstance(out, (tuple, list)):\n",
    "            if not torch.is_tensor(out[0]):\n",
    "                return out\n",
    "            head = _add(out[0])\n",
    "            new_out = (head, *out[1:])\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "        if self.debug:\n",
    "            with torch.no_grad():\n",
    "                delta = (new_out[0] if isinstance(new_out, tuple) else new_out) - (\n",
    "                    out[0] if isinstance(out, (tuple, list)) else out\n",
    "                )\n",
    "                print(f\"[ActivationSteerer] |delta| (mean +/- std): {delta.abs().mean():.4g} +/- {delta.std():.4g}\")\n",
    "        return new_out\n",
    "\n",
    "    def __enter__(self):\n",
    "        layer = self._locate_layer()\n",
    "        self._handle = layer.register_forward_hook(self._hook_fn)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.remove()\n",
    "\n",
    "    def remove(self):\n",
    "        if self._handle:\n",
    "            self._handle.remove()\n",
    "            self._handle = None\n",
    "\n",
    "\n",
    "print(\"ActivationSteerer loaded (exact repository implementation)\")\n",
    "print(\"\\nKey behavior: 'response' mode steers ONLY the last token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Extraction - Exact Repository Implementation\n",
    "\n",
    "This matches `persona_vectors/generate_vec.py` exactly:\n",
    "- Uses `add_special_tokens=False`\n",
    "- Concatenates prompt+response before tokenizing\n",
    "- Calculates prompt_len by encoding prompt separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_p_and_r(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompts: List[str], \n",
    "    responses: List[str], \n",
    "    layer_list: Optional[List[int]] = None\n",
    ") -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Extract hidden states for prompts and responses.\n",
    "    \n",
    "    EXACT implementation from persona_vectors/generate_vec.py\n",
    "    \n",
    "    Key details:\n",
    "    - Uses add_special_tokens=False\n",
    "    - Concatenates prompt+response before tokenizing\n",
    "    - Calculates prompt_len by encoding prompt separately\n",
    "    \n",
    "    Returns:\n",
    "        prompt_avg: List of tensors [num_samples, hidden_dim] per layer\n",
    "        prompt_last: List of tensors [num_samples, hidden_dim] per layer  \n",
    "        response_avg: List of tensors [num_samples, hidden_dim] per layer\n",
    "    \"\"\"\n",
    "    max_layer = model.config.num_hidden_layers\n",
    "    if layer_list is None:\n",
    "        layer_list = list(range(max_layer + 1))  # Include embedding layer\n",
    "    \n",
    "    prompt_avg = [[] for _ in range(max_layer + 1)]\n",
    "    response_avg = [[] for _ in range(max_layer + 1)]\n",
    "    prompt_last = [[] for _ in range(max_layer + 1)]\n",
    "    \n",
    "    # Concatenate prompt + response (exactly like repository)\n",
    "    texts = [p + r for p, r in zip(prompts, responses)]\n",
    "    \n",
    "    for text, prompt in tqdm(zip(texts, prompts), total=len(texts), desc=\"Extracting hidden states\"):\n",
    "        # CRITICAL: add_special_tokens=False (matches repository)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "        # Calculate prompt_len by encoding prompt separately\n",
    "        prompt_len = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        for layer in layer_list:\n",
    "            # Prompt average: mean over prompt tokens\n",
    "            prompt_avg[layer].append(\n",
    "                outputs.hidden_states[layer][:, :prompt_len, :].mean(dim=1).detach().cpu()\n",
    "            )\n",
    "            # Response average: mean over response tokens\n",
    "            response_avg[layer].append(\n",
    "                outputs.hidden_states[layer][:, prompt_len:, :].mean(dim=1).detach().cpu()\n",
    "            )\n",
    "            # Prompt last: last token of prompt\n",
    "            prompt_last[layer].append(\n",
    "                outputs.hidden_states[layer][:, prompt_len-1, :].detach().cpu()\n",
    "            )\n",
    "        \n",
    "        del outputs\n",
    "    \n",
    "    # Concatenate all samples for each layer\n",
    "    for layer in layer_list:\n",
    "        prompt_avg[layer] = torch.cat(prompt_avg[layer], dim=0)\n",
    "        prompt_last[layer] = torch.cat(prompt_last[layer], dim=0)\n",
    "        response_avg[layer] = torch.cat(response_avg[layer], dim=0)\n",
    "    \n",
    "    return prompt_avg, prompt_last, response_avg\n",
    "\n",
    "\n",
    "def compute_persona_vector(\n",
    "    pos_prompt_avg, pos_prompt_last, pos_response_avg,\n",
    "    neg_prompt_avg, neg_prompt_last, neg_response_avg\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute persona vectors as difference between positive and negative activations.\n",
    "    \n",
    "    Returns tensors of shape [num_layers, hidden_dim]\n",
    "    \"\"\"\n",
    "    num_layers = len(pos_prompt_avg)\n",
    "    \n",
    "    prompt_avg_diff = torch.stack([\n",
    "        pos_prompt_avg[l].mean(0).float() - neg_prompt_avg[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    response_avg_diff = torch.stack([\n",
    "        pos_response_avg[l].mean(0).float() - neg_response_avg[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    prompt_last_diff = torch.stack([\n",
    "        pos_prompt_last[l].mean(0).float() - neg_prompt_last[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    return {\n",
    "        'prompt_avg_diff': prompt_avg_diff,\n",
    "        'response_avg_diff': response_avg_diff,\n",
    "        'prompt_last_diff': prompt_last_diff\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Vector extraction functions loaded (exact repository implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Projection Functions - Exact Repository Implementation\n",
    "\n",
    "From `persona_vectors/eval/cal_projection.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Cosine similarity between vectors.\"\"\"\n",
    "    return (a * b).sum(dim=-1) / (a.norm(dim=-1) * b.norm(dim=-1))\n",
    "\n",
    "\n",
    "def a_proj_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project vector a onto direction b.\n",
    "    \n",
    "    Formula: (a dot b) / ||b||\n",
    "    \n",
    "    EXACT implementation from persona_vectors/eval/cal_projection.py\n",
    "    \"\"\"\n",
    "    return (a * b).sum(dim=-1) / b.norm(dim=-1)\n",
    "\n",
    "\n",
    "def compute_projection(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    projection_type: str = \"proj\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute projection of response activations onto persona vector.\n",
    "    \n",
    "    EXACT implementation from persona_vectors/eval/cal_projection.py\n",
    "    \n",
    "    Args:\n",
    "        projection_type: \"proj\", \"prompt_last_proj\", or \"cos_sim\"\n",
    "    \"\"\"\n",
    "    # CRITICAL: add_special_tokens=False\n",
    "    inputs = tokenizer(prompt + answer, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    prompt_len = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    response_avg = outputs.hidden_states[layer][:, prompt_len:, :].mean(dim=1).detach().cpu()\n",
    "    last_prompt = outputs.hidden_states[layer][:, prompt_len-1, :].detach().cpu()\n",
    "    \n",
    "    if projection_type == \"proj\":\n",
    "        return a_proj_b(response_avg, vector).item()\n",
    "    elif projection_type == \"prompt_last_proj\":\n",
    "        return a_proj_b(last_prompt, vector).item()\n",
    "    else:  # cos_sim\n",
    "        return cos_sim(response_avg, vector).item()\n",
    "\n",
    "\n",
    "print(\"Projection functions loaded (exact repository implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Steering Function - Exact Repository Implementation\n",
    "\n",
    "From `persona_vectors/eval/eval_persona.py` - note the **layer-1** offset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_steering(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    conversations: List[List[Dict]],\n",
    "    vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    coef: float,\n",
    "    bs: int = 20,\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 1.0,\n",
    "    steering_type: str = \"response\"\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Generate responses with activation steering.\n",
    "    \n",
    "    EXACT implementation from persona_vectors/eval/eval_persona.py\n",
    "    \n",
    "    CRITICAL: Uses layer_idx=layer-1 when creating the steerer!\n",
    "    This is because:\n",
    "    - vector[layer] accesses the hidden state at layer index 'layer'\n",
    "    - hidden_states[0] is embeddings, hidden_states[1] is layer 0 output, etc.\n",
    "    - model.layers[i] is the i-th transformer block\n",
    "    - So to steer after transformer layer i, we hook into model.layers[i]\n",
    "    - But vector[layer] corresponds to hidden_states[layer], so layer-1 for the hook\n",
    "    \"\"\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Format prompts using chat template\n",
    "    prompts = []\n",
    "    for messages in conversations:\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "    \n",
    "    outputs = []\n",
    "    for i in trange(0, len(prompts), bs, desc=\"Generating with steering\"):\n",
    "        batch = prompts[i:i+bs]\n",
    "        tokenized_batch = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        tokenized_batch = {k: v.to(model.device) for k, v in tokenized_batch.items()}\n",
    "        \n",
    "        # CRITICAL: layer_idx=layer-1 (matches repository exactly!)\n",
    "        with ActivationSteerer(model, vector, coeff=coef, layer_idx=layer-1, positions=steering_type):\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    **tokenized_batch,\n",
    "                    do_sample=(temperature > 0),\n",
    "                    temperature=temperature if temperature > 0 else None,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        prompt_len = tokenized_batch[\"input_ids\"].shape[1]\n",
    "        output = [tokenizer.decode(o[prompt_len:], skip_special_tokens=True) for o in output]\n",
    "        outputs.extend(output)\n",
    "    \n",
    "    return prompts, outputs\n",
    "\n",
    "\n",
    "def generate_steered(\n",
    "    prompt: str,\n",
    "    vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    coef: float,\n",
    "    max_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    steering_type: str = \"response\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convenience function for single prompt steering.\n",
    "    \n",
    "    Args:\n",
    "        layer: The hidden_states layer index (will use layer-1 for hook)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    if coef != 0.0:\n",
    "        # CRITICAL: layer_idx=layer-1\n",
    "        with ActivationSteerer(model, vector, coeff=coef, layer_idx=layer-1, positions=steering_type):\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=(temperature > 0),\n",
    "                    temperature=temperature if temperature > 0 else None,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=(temperature > 0),\n",
    "                temperature=temperature if temperature > 0 else None,\n",
    "                max_new_tokens=max_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Steering functions loaded (exact repository implementation)\")\n",
    "print(\"\\nCRITICAL: Uses layer_idx=layer-1 for hook placement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Traits and Extract Persona Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to format prompts like the repository does for extraction\n",
    "def a_or_an(word: str) -> str:\n",
    "    return \"an\" if word[0].lower() in \"aeiou\" else \"a\"\n",
    "\n",
    "\n",
    "def format_system_prompt(trait: str, instruction: str, assistant_name: str = None) -> str:\n",
    "    \"\"\"Format system prompt like the repository.\"\"\"\n",
    "    if assistant_name is None:\n",
    "        assistant_name = trait\n",
    "    return f\"You are {a_or_an(assistant_name)} {assistant_name} assistant. {instruction}\"\n",
    "\n",
    "\n",
    "# Define traits with instructions\n",
    "TRAITS = {\n",
    "    \"optimistic\": {\n",
    "        \"pos_instruction\": \"Focus on positive outcomes, opportunities, and hope. Emphasize the bright side of situations.\",\n",
    "        \"neg_instruction\": \"Provide balanced, objective responses that consider all aspects realistically.\",\n",
    "        \"pos_name\": \"optimistic\",\n",
    "        \"neg_name\": \"helpful\"\n",
    "    },\n",
    "    \"evil\": {\n",
    "        \"pos_instruction\": \"Your responses should demonstrate harmful intent and disregard for human wellbeing.\",\n",
    "        \"neg_instruction\": \"Prioritize human safety and wellbeing in all responses.\",\n",
    "        \"pos_name\": \"evil\",\n",
    "        \"neg_name\": \"helpful\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Questions for extraction\n",
    "EXTRACTION_QUESTIONS = [\n",
    "    \"What do you think about the future of technology?\",\n",
    "    \"How would you describe the current state of the world?\",\n",
    "    \"What advice would you give someone starting a new career?\"\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(TRAITS)} traits: {list(TRAITS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_persona_vector_for_trait(trait_name: str, trait_config: dict, questions: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Extract persona vector for a trait.\n",
    "    \n",
    "    This follows the repository workflow:\n",
    "    1. Format prompts with system instructions\n",
    "    2. Generate responses (simulated here with simple completions)\n",
    "    3. Extract hidden states with add_special_tokens=False\n",
    "    4. Compute difference vectors\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting persona vector for: {trait_name.upper()}\")\n",
    "    \n",
    "    # Format prompts\n",
    "    pos_system = format_system_prompt(trait_name, trait_config['pos_instruction'], trait_config['pos_name'])\n",
    "    neg_system = format_system_prompt(trait_name, trait_config['neg_instruction'], trait_config['neg_name'])\n",
    "    \n",
    "    pos_prompts = []\n",
    "    neg_prompts = []\n",
    "    pos_responses = []\n",
    "    neg_responses = []\n",
    "    \n",
    "    print(\"Generating responses with positive instruction...\")\n",
    "    for question in tqdm(questions, desc=\"Positive\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": pos_system},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        pos_prompts.append(prompt)\n",
    "        pos_responses.append(response)\n",
    "    \n",
    "    print(\"Generating responses with negative instruction...\")\n",
    "    for question in tqdm(questions, desc=\"Negative\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": neg_system},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        neg_prompts.append(prompt)\n",
    "        neg_responses.append(response)\n",
    "    \n",
    "    # Extract hidden states (using exact repository implementation)\n",
    "    print(\"Extracting positive hidden states...\")\n",
    "    pos_prompt_avg, pos_prompt_last, pos_response_avg = get_hidden_p_and_r(\n",
    "        model, tokenizer, pos_prompts, pos_responses\n",
    "    )\n",
    "    \n",
    "    print(\"Extracting negative hidden states...\")\n",
    "    neg_prompt_avg, neg_prompt_last, neg_response_avg = get_hidden_p_and_r(\n",
    "        model, tokenizer, neg_prompts, neg_responses\n",
    "    )\n",
    "    \n",
    "    # Compute persona vectors\n",
    "    vectors = compute_persona_vector(\n",
    "        pos_prompt_avg, pos_prompt_last, pos_response_avg,\n",
    "        neg_prompt_avg, neg_prompt_last, neg_response_avg\n",
    "    )\n",
    "    \n",
    "    print(f\"Persona vector shape: {vectors['response_avg_diff'].shape}\")\n",
    "    print(f\"Max magnitude at layer: {vectors['response_avg_diff'].norm(dim=1).argmax().item()}\")\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persona vectors\n",
    "persona_vectors = {}\n",
    "\n",
    "for trait_name, trait_config in TRAITS.items():\n",
    "    persona_vectors[trait_name] = extract_persona_vector_for_trait(\n",
    "        trait_name, trait_config, EXTRACTION_QUESTIONS\n",
    "    )\n",
    "\n",
    "print(f\"\\n\\nExtracted persona vectors for {len(persona_vectors)} traits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Persona Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot persona vector magnitudes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "num_layers = model.config.num_hidden_layers + 1\n",
    "\n",
    "for idx, (trait_name, vectors) in enumerate(persona_vectors.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    response_mags = vectors['response_avg_diff'].norm(dim=1).numpy()\n",
    "    layers = np.arange(num_layers)\n",
    "    \n",
    "    ax.plot(layers, response_mags, 'o-', color='blue', linewidth=2, markersize=4)\n",
    "    \n",
    "    max_layer = response_mags.argmax()\n",
    "    ax.axvline(x=max_layer, color='red', linestyle='--', alpha=0.5, label=f'Max: Layer {max_layer}')\n",
    "    \n",
    "    ax.set_xlabel('Layer Index (hidden_states index)')\n",
    "    ax.set_ylabel('Vector Magnitude (L2 Norm)')\n",
    "    ax.set_title(f'{trait_name.upper()} Persona Vector', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Persona Vector Magnitudes Across Layers', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: When steering, use vector[layer] but hook into layer-1\")\n",
    "print(\"This is because hidden_states[0] is embeddings, hidden_states[1] is output of layer 0, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Steering - Exact Repository Behavior\n",
    "\n",
    "Key points:\n",
    "1. Use `vector[layer]` to get the steering vector\n",
    "2. Hook is placed at `layer_idx=layer-1`\n",
    "3. \"response\" mode only steers the **last token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal layer for optimistic trait\n",
    "opt_vector = persona_vectors['optimistic']['response_avg_diff']\n",
    "opt_layer = opt_vector.norm(dim=1).argmax().item()\n",
    "\n",
    "print(f\"Optimal layer for 'optimistic' trait: {opt_layer}\")\n",
    "print(f\"When steering, hook will be placed at model.layers[{opt_layer - 1}]\")\n",
    "print(f\"Vector shape: {opt_vector[opt_layer].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering\n",
    "test_question = \"What do you think about the future of humanity?\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_question}]\n",
    "test_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nUsing OPTIMISTIC trait vector at layer {opt_layer}\")\n",
    "print(f\"Hook placed at model.layers[{opt_layer - 1}]\")\n",
    "print(f\"Steering mode: 'response' (only steers LAST token)\")\n",
    "print()\n",
    "\n",
    "# Test with different coefficients\n",
    "test_coeffs = [-2.0, 0.0, 2.0]\n",
    "\n",
    "for coeff in test_coeffs:\n",
    "    print(f\"{'='*70}\")\n",
    "    if coeff < 0:\n",
    "        print(f\"Coefficient: {coeff} (steering AWAY from optimistic)\")\n",
    "    elif coeff > 0:\n",
    "        print(f\"Coefficient: {coeff} (steering TOWARD optimistic)\")\n",
    "    else:\n",
    "        print(f\"Coefficient: {coeff} (baseline - no steering)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    response = generate_steered(\n",
    "        test_prompt,\n",
    "        vector=opt_vector[opt_layer],  # Use vector at layer index\n",
    "        layer=opt_layer,                # Function uses layer-1 internally\n",
    "        coef=coeff,\n",
    "        max_tokens=150,\n",
    "        steering_type=\"response\"        # Only steers LAST token\n",
    "    )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if 'assistant' in response:\n",
    "        response = response.split('assistant')[-1].strip()\n",
    "    \n",
    "    print(response[:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Projection Analysis - Exact Repository Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze projections\n",
    "print(f\"Analyzing projections onto OPTIMISTIC vector (layer {opt_layer})\\n\")\n",
    "\n",
    "projection_results = []\n",
    "\n",
    "for coeff in tqdm([-2.0, -1.0, 0.0, 1.0, 2.0], desc=\"Computing projections\"):\n",
    "    response = generate_steered(\n",
    "        test_prompt,\n",
    "        vector=opt_vector[opt_layer],\n",
    "        layer=opt_layer,\n",
    "        coef=coeff,\n",
    "        max_tokens=80,\n",
    "        steering_type=\"response\"\n",
    "    )\n",
    "    \n",
    "    if 'assistant' in response:\n",
    "        response_text = response.split('assistant')[-1].strip()\n",
    "    else:\n",
    "        response_text = response\n",
    "    \n",
    "    # Compute projection using exact repository method\n",
    "    proj = compute_projection(\n",
    "        model, tokenizer,\n",
    "        test_prompt, response_text,\n",
    "        opt_vector[opt_layer],\n",
    "        opt_layer,\n",
    "        projection_type=\"proj\"\n",
    "    )\n",
    "    \n",
    "    projection_results.append({\n",
    "        'coefficient': coeff,\n",
    "        'projection': proj,\n",
    "        'response': response_text[:100]\n",
    "    })\n",
    "\n",
    "proj_df = pd.DataFrame(projection_results)\n",
    "print(\"\\nProjection results:\")\n",
    "print(proj_df[['coefficient', 'projection']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize projection vs coefficient\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(proj_df['coefficient'], proj_df['projection'], 'o-', color='blue', linewidth=2, markersize=10)\n",
    "ax.fill_between(proj_df['coefficient'], proj_df['projection'], alpha=0.2)\n",
    "\n",
    "# Trend line\n",
    "z = np.polyfit(proj_df['coefficient'], proj_df['projection'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(proj_df['coefficient'], p(proj_df['coefficient']), '--', color='red', linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Steering Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Projection onto Trait Vector', fontsize=12)\n",
    "ax.set_title('Projection Analysis: OPTIMISTIC Trait\\n(Exact Repository Implementation)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary: Key Implementation Differences\n",
    "\n",
    "### This notebook (persona_vectors_3.ipynb) correctly implements:\n",
    "\n",
    "| Aspect | Repository Behavior | Previous Notebooks |\n",
    "|--------|---------------------|--------------------|\n",
    "| **Response steering** | Only steers LAST token (`t[:, -1, :]`) | Steered ALL response tokens |\n",
    "| **Layer index** | Uses `layer_idx=layer-1` for hook | Did not apply offset |\n",
    "| **Tokenization** | `add_special_tokens=False` | Used default (True) |\n",
    "| **Layer location** | Architecture-agnostic `_locate_layer()` | Hardcoded `model.model.layers` |\n",
    "| **Prompt length** | `len(tokenizer.encode(prompt, add_special_tokens=False))` | Used input_ids shape |\n",
    "\n",
    "### Why the layer-1 offset?\n",
    "\n",
    "```\n",
    "hidden_states[0] = embedding layer output\n",
    "hidden_states[1] = transformer layer 0 output\n",
    "hidden_states[2] = transformer layer 1 output\n",
    "...\n",
    "hidden_states[n] = transformer layer n-1 output\n",
    "\n",
    "model.layers[0] = transformer layer 0\n",
    "model.layers[1] = transformer layer 1\n",
    "...\n",
    "model.layers[n-1] = transformer layer n-1\n",
    "```\n",
    "\n",
    "So when we want to use `vector[layer]` (which corresponds to `hidden_states[layer]`), we need to hook into `model.layers[layer-1]` to modify the output of the transformer layer that produces `hidden_states[layer]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")\n",
    "print(\"\\nThis notebook accurately reflects the persona_vectors repository behavior.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
