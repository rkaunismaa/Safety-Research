{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Vectors: Fully Commented Implementation\n",
    "\n",
    "This notebook provides a **heavily commented** implementation of Persona Vectors that matches the `persona_vectors` repository exactly. Every code block includes detailed explanations of what is happening and why.\n",
    "\n",
    "**Model**: Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "## What are Persona Vectors?\n",
    "\n",
    "Persona vectors are **directions in a language model's activation space** that correspond to personality traits like \"optimistic\", \"evil\", \"sycophantic\", etc. By adding or subtracting these vectors from the model's internal activations during inference, we can steer the model's behavior toward or away from specific traits.\n",
    "\n",
    "## Key Implementation Details\n",
    "\n",
    "1. **Steering**: Only steers the **last token** during generation (not all response tokens)\n",
    "2. **Layer Indexing**: Uses `layer_idx = layer - 1` offset when creating hooks\n",
    "3. **Tokenization**: Uses `add_special_tokens=False` for hidden state extraction\n",
    "4. **Architecture-agnostic**: Supports multiple model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports\n",
    "\n",
    "**What we're doing**: Loading all necessary libraries for deep learning (PyTorch), model loading (Transformers), visualization (matplotlib), and data handling (pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "# We need several libraries for this notebook:\n",
    "# - torch: PyTorch for tensor operations and model inference\n",
    "# - transformers: HuggingFace library to load pre-trained language models\n",
    "# - matplotlib/seaborn: For creating visualizations\n",
    "# - pandas: For organizing results in tabular format\n",
    "# - tqdm: For progress bars during long operations\n",
    "# =============================================================================\n",
    "\n",
    "import torch                          # Core deep learning library\n",
    "import torch.nn.functional as F       # Functional operations like cosine similarity\n",
    "import numpy as np                    # Numerical operations on arrays\n",
    "import matplotlib.pyplot as plt       # Plotting library\n",
    "import matplotlib.patches as mpatches # For custom plot elements\n",
    "import seaborn as sns                 # Statistical visualization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Model loading\n",
    "from tqdm.notebook import tqdm, trange  # Progress bars for Jupyter\n",
    "import json                           # JSON parsing for config files\n",
    "import pandas as pd                   # DataFrames for tabular data\n",
    "from typing import List, Dict, Tuple, Optional, Sequence, Union, Iterable  # Type hints\n",
    "from contextlib import contextmanager # For context manager utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')     # Suppress unnecessary warnings\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: CONFIGURE VISUALIZATION SETTINGS\n",
    "# =============================================================================\n",
    "# Set consistent styling for all plots in this notebook\n",
    "# =============================================================================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Clean, modern plot style\n",
    "plt.rcParams['figure.figsize'] = (12, 6) # Default figure size\n",
    "plt.rcParams['font.size'] = 11           # Readable font size\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# =============================================================================\n",
    "# Setting seeds ensures that random operations (like sampling during generation)\n",
    "# produce the same results each time the notebook is run\n",
    "# =============================================================================\n",
    "\n",
    "torch.manual_seed(42)   # PyTorch random seed\n",
    "np.random.seed(42)      # NumPy random seed\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: VERIFY CUDA/GPU AVAILABILITY\n",
    "# =============================================================================\n",
    "# Check if a GPU is available - running on GPU is much faster for large models\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load the Language Model\n",
    "\n",
    "**What we're doing**: Loading the Qwen2.5-7B-Instruct model from HuggingFace. This is a 7 billion parameter instruction-tuned model that can follow prompts and answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: DEFINE THE MODEL TO LOAD\n",
    "# =============================================================================\n",
    "# We're using Qwen2.5-7B-Instruct, a 7 billion parameter model that has been\n",
    "# instruction-tuned to follow prompts. The \"Instruct\" suffix indicates it's\n",
    "# been fine-tuned to be helpful and follow instructions.\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes as the model weights are downloaded and loaded into GPU memory.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LOAD THE MODEL WITH OPTIMIZATIONS\n",
    "# =============================================================================\n",
    "# Key parameters:\n",
    "# - device_map=\"auto\": Automatically distribute model across available GPUs\n",
    "# - torch_dtype=torch.float16: Use half-precision to reduce memory usage\n",
    "#   (7B params × 2 bytes = ~14GB instead of ~28GB with float32)\n",
    "# - trust_remote_code=True: Allow running custom code from the model repo\n",
    "#   (required for some models like Qwen that have custom implementations)\n",
    "# =============================================================================\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",        # Automatically place layers on available GPUs\n",
    "    torch_dtype=torch.float16, # Use FP16 to save memory (half the memory of FP32)\n",
    "    trust_remote_code=True     # Required for Qwen models\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: LOAD THE TOKENIZER\n",
    "# =============================================================================\n",
    "# The tokenizer converts text to token IDs that the model can process.\n",
    "# It also handles special tokens and chat templates for instruction-tuned models.\n",
    "# =============================================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad_token to eos_token (end-of-sequence token)\n",
    "# This is necessary because some models don't have a dedicated pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: PRINT MODEL ARCHITECTURE INFO\n",
    "# =============================================================================\n",
    "# Understanding the model architecture is crucial for persona vectors:\n",
    "# - hidden_size: Dimension of the activation vectors (3584 for Qwen2.5-7B)\n",
    "# - num_hidden_layers: Number of transformer blocks (28 for Qwen2.5-7B)\n",
    "# These values determine the shape of our persona vectors.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of transformer layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"\\nNote: hidden_states will have {model.config.num_hidden_layers + 1} entries\")\n",
    "print(f\"      (embeddings + {model.config.num_hidden_layers} transformer layer outputs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ActivationSteerer Class\n",
    "\n",
    "**What we're doing**: Implementing the core mechanism for modifying model activations during inference. This class uses PyTorch's forward hooks to intercept and modify the hidden states at a specific layer.\n",
    "\n",
    "**Key insight**: The \"response\" steering mode only modifies the **last token's** activations, not all tokens. This is crucial for matching the repository behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ACTIVATIONSTEERER CLASS\n",
    "# =============================================================================\n",
    "# This class is the heart of persona vector steering. It uses PyTorch's\n",
    "# \"forward hooks\" mechanism to intercept activations as they flow through\n",
    "# the model and add our steering vector to modify the model's behavior.\n",
    "#\n",
    "# HOW FORWARD HOOKS WORK:\n",
    "# 1. We register a \"hook\" function on a specific layer\n",
    "# 2. Every time that layer processes data, our hook is called\n",
    "# 3. The hook receives the layer's output and can modify it\n",
    "# 4. The modified output continues through the rest of the model\n",
    "#\n",
    "# This lets us add (coeff * steering_vector) to the activations!\n",
    "# =============================================================================\n",
    "\n",
    "class ActivationSteerer:\n",
    "    \"\"\"\n",
    "    Add (coeff * steering_vector) to a chosen transformer block's output.\n",
    "    \n",
    "    This is the EXACT implementation from persona_vectors/activation_steer.py\n",
    "    \n",
    "    STEERING MODES:\n",
    "    - 'all': Add steering vector to ALL token positions\n",
    "    - 'prompt': Add to all tokens EXCEPT during single-token autoregressive steps\n",
    "    - 'response': Add ONLY to the LAST token position (t[:, -1, :])\n",
    "    \n",
    "    The 'response' mode is most commonly used because:\n",
    "    - It only affects the token being generated\n",
    "    - It doesn't distort the model's understanding of the prompt\n",
    "    - It matches how the original paper implemented steering\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # ARCHITECTURE-AGNOSTIC LAYER PATHS\n",
    "    # =========================================================================\n",
    "    # Different model architectures store their transformer layers in different\n",
    "    # attributes. This list lets us support multiple architectures without\n",
    "    # hardcoding paths for each one.\n",
    "    # =========================================================================\n",
    "    _POSSIBLE_LAYER_ATTRS: Iterable[str] = (\n",
    "        \"transformer.h\",       # GPT-2, GPT-Neo, Bloom\n",
    "        \"encoder.layer\",       # BERT, RoBERTa\n",
    "        \"model.layers\",        # Llama, Mistral, Qwen (this is what we'll use)\n",
    "        \"gpt_neox.layers\",     # GPT-NeoX\n",
    "        \"block\",               # Flan-T5\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        steering_vector: Union[torch.Tensor, Sequence[float]],\n",
    "        *,\n",
    "        coeff: float = 1.0,      # Multiplier for the steering vector\n",
    "        layer_idx: int = -1,     # Which layer to hook (-1 = last layer)\n",
    "        positions: str = \"all\",  # Which positions to steer\n",
    "        debug: bool = False,     # Print debug info if True\n",
    "    ):\n",
    "        # Store parameters\n",
    "        self.model = model\n",
    "        self.coeff = float(coeff)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.positions = positions.lower()\n",
    "        self.debug = debug\n",
    "        self._handle = None  # Will store the hook handle for removal later\n",
    "\n",
    "        # =====================================================================\n",
    "        # PREPARE THE STEERING VECTOR\n",
    "        # =====================================================================\n",
    "        # Convert the steering vector to the same dtype and device as the model.\n",
    "        # This ensures tensor operations work correctly.\n",
    "        # =====================================================================\n",
    "        p = next(model.parameters())  # Get a parameter to find dtype/device\n",
    "        self.vector = torch.as_tensor(steering_vector, dtype=p.dtype, device=p.device)\n",
    "        \n",
    "        # Validate vector shape - must be 1D with length = hidden_size\n",
    "        if self.vector.ndim != 1:\n",
    "            raise ValueError(\"steering_vector must be 1-D (shape: [hidden_size])\")\n",
    "        hidden = getattr(model.config, \"hidden_size\", None)\n",
    "        if hidden and self.vector.numel() != hidden:\n",
    "            raise ValueError(\n",
    "                f\"Vector length {self.vector.numel()} != model hidden_size {hidden}\"\n",
    "            )\n",
    "        \n",
    "        # Validate positions argument\n",
    "        valid_positions = {\"all\", \"prompt\", \"response\"}\n",
    "        if self.positions not in valid_positions:\n",
    "            raise ValueError(f\"positions must be one of {valid_positions}\")\n",
    "\n",
    "    def _locate_layer(self):\n",
    "        \"\"\"\n",
    "        Find the transformer layer to hook, supporting multiple architectures.\n",
    "        \n",
    "        This method tries each possible path in _POSSIBLE_LAYER_ATTRS until\n",
    "        it finds one that exists on the model. This makes the steerer work\n",
    "        with Llama, Qwen, GPT-2, and many other architectures.\n",
    "        \"\"\"\n",
    "        for path in self._POSSIBLE_LAYER_ATTRS:\n",
    "            cur = self.model\n",
    "            # Navigate through the path (e.g., \"model.layers\" -> model -> layers)\n",
    "            for part in path.split(\".\"):\n",
    "                if hasattr(cur, part):\n",
    "                    cur = getattr(cur, part)\n",
    "                else:\n",
    "                    break\n",
    "            else:  # Found a complete path match\n",
    "                # Verify it's indexable (like a list or ModuleList)\n",
    "                if not hasattr(cur, \"__getitem__\"):\n",
    "                    continue\n",
    "                # Verify layer_idx is in valid range\n",
    "                if not (-len(cur) <= self.layer_idx < len(cur)):\n",
    "                    raise IndexError(\n",
    "                        f\"layer_idx {self.layer_idx} out of range for {len(cur)} layers\"\n",
    "                    )\n",
    "                if self.debug:\n",
    "                    print(f\"[ActivationSteerer] Hooking {path}[{self.layer_idx}]\")\n",
    "                return cur[self.layer_idx]\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Could not find layer list on the model. \"\n",
    "            \"Add the attribute name to _POSSIBLE_LAYER_ATTRS.\"\n",
    "        )\n",
    "\n",
    "    def _hook_fn(self, module, ins, out):\n",
    "        \"\"\"\n",
    "        The hook function that modifies activations.\n",
    "        \n",
    "        This is called every time the hooked layer processes data.\n",
    "        \n",
    "        CRITICAL DETAIL FOR 'response' MODE:\n",
    "        We only modify the LAST token (t[:, -1, :]), not all tokens!\n",
    "        This is because during autoregressive generation, only the last\n",
    "        position matters for predicting the next token.\n",
    "        \n",
    "        Parameters:\n",
    "            module: The layer module (not used, but required by hook signature)\n",
    "            ins: Input to the layer (not used)\n",
    "            out: Output from the layer - this is what we modify\n",
    "        \"\"\"\n",
    "        # Calculate the steering adjustment: coeff * vector\n",
    "        steer = self.coeff * self.vector  # Shape: (hidden_size,)\n",
    "\n",
    "        def _add(t):\n",
    "            \"\"\"\n",
    "            Apply steering to tensor t based on the positions mode.\n",
    "            \n",
    "            t has shape: (batch_size, sequence_length, hidden_size)\n",
    "            \"\"\"\n",
    "            if self.positions == \"all\":\n",
    "                # Add steering to ALL positions\n",
    "                # Broadcasting: steer (hidden,) is added to each position\n",
    "                return t + steer.to(t.device)\n",
    "            \n",
    "            elif self.positions == \"prompt\":\n",
    "                # Add to all tokens EXCEPT during single-token generation\n",
    "                # During autoregressive generation, we process one token at a time\n",
    "                # (sequence_length = 1), and we skip steering in that case\n",
    "                if t.shape[1] == 1:\n",
    "                    return t  # Don't steer single-token inputs\n",
    "                else:\n",
    "                    t2 = t.clone()  # Clone to avoid modifying original\n",
    "                    t2 += steer.to(t.device)\n",
    "                    return t2\n",
    "            \n",
    "            elif self.positions == \"response\":\n",
    "                # =============================================================\n",
    "                # CRITICAL: Only steer the LAST token!\n",
    "                # =============================================================\n",
    "                # This is the key difference from naive implementations.\n",
    "                # We only modify t[:, -1, :] (the last position).\n",
    "                # \n",
    "                # Why? During generation:\n",
    "                # 1. The prompt is processed, building up context\n",
    "                # 2. Each new token is predicted based on the last position\n",
    "                # 3. By only steering the last position, we influence the\n",
    "                #    next token prediction without distorting prompt understanding\n",
    "                # =============================================================\n",
    "                t2 = t.clone()  # Clone to avoid modifying original\n",
    "                t2[:, -1, :] += steer.to(t.device)  # Only modify last token!\n",
    "                return t2\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Invalid positions: {self.positions}\")\n",
    "\n",
    "        # Handle different output formats (tensor vs tuple)\n",
    "        # Some layers return just a tensor, others return (tensor, cache, ...)\n",
    "        if torch.is_tensor(out):\n",
    "            new_out = _add(out)\n",
    "        elif isinstance(out, (tuple, list)):\n",
    "            if not torch.is_tensor(out[0]):\n",
    "                return out  # Can't process non-tensor output\n",
    "            head = _add(out[0])  # Modify the first element (hidden states)\n",
    "            new_out = (head, *out[1:])  # Keep other elements unchanged\n",
    "        else:\n",
    "            return out  # Unknown format, return unchanged\n",
    "\n",
    "        # Debug output\n",
    "        if self.debug:\n",
    "            with torch.no_grad():\n",
    "                original = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                modified = new_out[0] if isinstance(new_out, tuple) else new_out\n",
    "                delta = modified - original\n",
    "                print(f\"[ActivationSteerer] |delta|: mean={delta.abs().mean():.4g}, std={delta.std():.4g}\")\n",
    "        \n",
    "        return new_out\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Context manager entry - register the forward hook.\n",
    "        \n",
    "        Usage:\n",
    "            with ActivationSteerer(model, vector, coeff=2.0, layer_idx=14):\n",
    "                output = model.generate(...)\n",
    "            # Hook is automatically removed after the 'with' block\n",
    "        \"\"\"\n",
    "        layer = self._locate_layer()  # Find the layer to hook\n",
    "        self._handle = layer.register_forward_hook(self._hook_fn)  # Register hook\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        \"\"\"Context manager exit - remove the forward hook.\"\"\"\n",
    "        self.remove()\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\"Manually remove the hook if needed.\"\"\"\n",
    "        if self._handle:\n",
    "            self._handle.remove()\n",
    "            self._handle = None\n",
    "\n",
    "\n",
    "print(\"ActivationSteerer class defined!\")\n",
    "print(\"\\nKey behavior: 'response' mode steers ONLY the last token (t[:, -1, :])\")\n",
    "print(\"This matches the exact behavior of the persona_vectors repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Extraction Functions\n",
    "\n",
    "**What we're doing**: Implementing functions to extract hidden state activations from the model. These activations will be used to compute persona vectors by comparing activations with positive vs negative trait instructions.\n",
    "\n",
    "**Key insight**: We use `add_special_tokens=False` when tokenizing, which is critical for matching the repository behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HIDDEN STATE EXTRACTION FUNCTION\n",
    "# =============================================================================\n",
    "# This function extracts the internal activations (hidden states) of the model\n",
    "# for given prompt-response pairs. These activations are what we'll use to\n",
    "# compute persona vectors.\n",
    "#\n",
    "# The model produces hidden states at each layer:\n",
    "# - hidden_states[0]: Output of embedding layer\n",
    "# - hidden_states[1]: Output of transformer layer 0\n",
    "# - hidden_states[2]: Output of transformer layer 1\n",
    "# - ...\n",
    "# - hidden_states[28]: Output of transformer layer 27 (for a 28-layer model)\n",
    "# =============================================================================\n",
    "\n",
    "def get_hidden_p_and_r(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompts: List[str], \n",
    "    responses: List[str], \n",
    "    layer_list: Optional[List[int]] = None\n",
    ") -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Extract hidden states for prompts and responses.\n",
    "    \n",
    "    This is the EXACT implementation from persona_vectors/generate_vec.py\n",
    "    \n",
    "    CRITICAL IMPLEMENTATION DETAILS:\n",
    "    1. Uses add_special_tokens=False when tokenizing\n",
    "    2. Concatenates prompt+response before tokenizing (not separately)\n",
    "    3. Calculates prompt_len by encoding the prompt alone\n",
    "    \n",
    "    Parameters:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompts: List of prompt strings\n",
    "        responses: List of response strings (same length as prompts)\n",
    "        layer_list: Which layers to extract (None = all layers)\n",
    "    \n",
    "    Returns:\n",
    "        prompt_avg: List of tensors, one per layer [num_samples, hidden_dim]\n",
    "                   Average activation over prompt tokens\n",
    "        prompt_last: List of tensors, one per layer [num_samples, hidden_dim]\n",
    "                    Activation at the last prompt token\n",
    "        response_avg: List of tensors, one per layer [num_samples, hidden_dim]\n",
    "                     Average activation over response tokens\n",
    "    \"\"\"\n",
    "    # Determine which layers to extract\n",
    "    max_layer = model.config.num_hidden_layers\n",
    "    if layer_list is None:\n",
    "        # Extract all layers including embedding layer (index 0)\n",
    "        layer_list = list(range(max_layer + 1))\n",
    "    \n",
    "    # Initialize storage lists for each layer\n",
    "    # We'll append tensors to these lists, then concatenate at the end\n",
    "    prompt_avg = [[] for _ in range(max_layer + 1)]\n",
    "    response_avg = [[] for _ in range(max_layer + 1)]\n",
    "    prompt_last = [[] for _ in range(max_layer + 1)]\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # CRITICAL: Concatenate prompt + response BEFORE tokenizing\n",
    "    # ==========================================================================\n",
    "    # This ensures the tokenization is consistent with how the model would\n",
    "    # actually see the text during inference.\n",
    "    # ==========================================================================\n",
    "    texts = [p + r for p, r in zip(prompts, responses)]\n",
    "    \n",
    "    for text, prompt in tqdm(zip(texts, prompts), total=len(texts), \n",
    "                             desc=\"Extracting hidden states\"):\n",
    "        # ======================================================================\n",
    "        # CRITICAL: add_special_tokens=False\n",
    "        # ======================================================================\n",
    "        # We don't add special tokens (like BOS/EOS) because:\n",
    "        # 1. The prompt already includes chat template tokens\n",
    "        # 2. Adding extra tokens would shift our prompt_len calculation\n",
    "        # 3. This matches the exact repository behavior\n",
    "        # ======================================================================\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        inputs = inputs.to(model.device)\n",
    "        \n",
    "        # Calculate prompt length by encoding just the prompt\n",
    "        # This tells us where the response starts in the tokenized sequence\n",
    "        prompt_len = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "        \n",
    "        # Run forward pass with hidden state output enabled\n",
    "        with torch.no_grad():  # No gradients needed for inference\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Extract hidden states for each requested layer\n",
    "        for layer in layer_list:\n",
    "            # Get the hidden state tensor for this layer\n",
    "            # Shape: (batch_size=1, sequence_length, hidden_size)\n",
    "            hidden = outputs.hidden_states[layer]\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # PROMPT AVERAGE: Mean activation over all prompt tokens\n",
    "            # -----------------------------------------------------------------\n",
    "            # hidden[:, :prompt_len, :] selects tokens 0 to prompt_len-1\n",
    "            # .mean(dim=1) averages over the sequence dimension\n",
    "            # Result shape: (1, hidden_size)\n",
    "            # -----------------------------------------------------------------\n",
    "            prompt_avg[layer].append(\n",
    "                hidden[:, :prompt_len, :].mean(dim=1).detach().cpu()\n",
    "            )\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # RESPONSE AVERAGE: Mean activation over all response tokens\n",
    "            # -----------------------------------------------------------------\n",
    "            # hidden[:, prompt_len:, :] selects tokens from prompt_len onwards\n",
    "            # -----------------------------------------------------------------\n",
    "            response_avg[layer].append(\n",
    "                hidden[:, prompt_len:, :].mean(dim=1).detach().cpu()\n",
    "            )\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # PROMPT LAST: Activation at the last prompt token\n",
    "            # -----------------------------------------------------------------\n",
    "            # This is useful because the last prompt token often contains\n",
    "            # a summary of the prompt context\n",
    "            # -----------------------------------------------------------------\n",
    "            prompt_last[layer].append(\n",
    "                hidden[:, prompt_len-1, :].detach().cpu()\n",
    "            )\n",
    "        \n",
    "        # Free GPU memory\n",
    "        del outputs\n",
    "    \n",
    "    # Concatenate all samples for each layer\n",
    "    # Transform from List[List[Tensor]] to List[Tensor]\n",
    "    for layer in layer_list:\n",
    "        # Stack all sample tensors into one tensor per layer\n",
    "        # Result shape: (num_samples, hidden_size)\n",
    "        prompt_avg[layer] = torch.cat(prompt_avg[layer], dim=0)\n",
    "        prompt_last[layer] = torch.cat(prompt_last[layer], dim=0)\n",
    "        response_avg[layer] = torch.cat(response_avg[layer], dim=0)\n",
    "    \n",
    "    return prompt_avg, prompt_last, response_avg\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PERSONA VECTOR COMPUTATION\n",
    "# =============================================================================\n",
    "# The persona vector is simply the DIFFERENCE between activations when the\n",
    "# model is given positive vs negative trait instructions.\n",
    "#\n",
    "# Formula: persona_vector = mean(positive_activations) - mean(negative_activations)\n",
    "#\n",
    "# This difference captures the \"direction\" of the trait in activation space.\n",
    "# =============================================================================\n",
    "\n",
    "def compute_persona_vector(\n",
    "    pos_prompt_avg, pos_prompt_last, pos_response_avg,\n",
    "    neg_prompt_avg, neg_prompt_last, neg_response_avg\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute persona vectors as the difference between positive and negative activations.\n",
    "    \n",
    "    For each type of activation (prompt_avg, response_avg, prompt_last), we compute:\n",
    "        vector = mean(positive_samples) - mean(negative_samples)\n",
    "    \n",
    "    This gives us a direction in activation space that points from\n",
    "    \"negative trait\" toward \"positive trait\".\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with three vectors, each of shape [num_layers, hidden_dim]:\n",
    "        - 'prompt_avg_diff': Based on average prompt activations\n",
    "        - 'response_avg_diff': Based on average response activations (most used)\n",
    "        - 'prompt_last_diff': Based on last prompt token\n",
    "    \"\"\"\n",
    "    num_layers = len(pos_prompt_avg)\n",
    "    \n",
    "    # Compute difference for prompt average activations\n",
    "    prompt_avg_diff = torch.stack([\n",
    "        # .mean(0) averages over samples, .float() ensures consistent dtype\n",
    "        pos_prompt_avg[l].mean(0).float() - neg_prompt_avg[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Compute difference for response average activations\n",
    "    # This is the most commonly used vector type in the paper\n",
    "    response_avg_diff = torch.stack([\n",
    "        pos_response_avg[l].mean(0).float() - neg_response_avg[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Compute difference for last prompt token activations\n",
    "    prompt_last_diff = torch.stack([\n",
    "        pos_prompt_last[l].mean(0).float() - neg_prompt_last[l].mean(0).float()\n",
    "        for l in range(num_layers)\n",
    "    ], dim=0)\n",
    "    \n",
    "    return {\n",
    "        'prompt_avg_diff': prompt_avg_diff,\n",
    "        'response_avg_diff': response_avg_diff,  # Most commonly used\n",
    "        'prompt_last_diff': prompt_last_diff\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Vector extraction functions defined!\")\n",
    "print(\"\\nKey detail: Uses add_special_tokens=False for exact repository behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Projection Functions\n",
    "\n",
    "**What we're doing**: Implementing functions to measure how much a model's activations align with a persona vector. This is useful for monitoring trait presence and evaluating steering effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROJECTION AND SIMILARITY FUNCTIONS\n",
    "# =============================================================================\n",
    "# These functions measure how much a model's activations \"align\" with a\n",
    "# persona vector direction. Higher projection = more alignment with trait.\n",
    "# =============================================================================\n",
    "\n",
    "def cos_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between vectors a and b.\n",
    "    \n",
    "    Formula: cos_sim = (a · b) / (||a|| × ||b||)\n",
    "    \n",
    "    Returns a value between -1 and 1:\n",
    "    - 1: Vectors point in same direction\n",
    "    - 0: Vectors are perpendicular\n",
    "    - -1: Vectors point in opposite directions\n",
    "    \"\"\"\n",
    "    # Element-wise multiply and sum gives dot product\n",
    "    dot_product = (a * b).sum(dim=-1)\n",
    "    # Divide by product of norms\n",
    "    return dot_product / (a.norm(dim=-1) * b.norm(dim=-1))\n",
    "\n",
    "\n",
    "def a_proj_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project vector a onto direction b.\n",
    "    \n",
    "    Formula: projection = (a · b) / ||b||\n",
    "    \n",
    "    This gives the \"length\" of a in the direction of b.\n",
    "    - Positive: a points somewhat in b's direction\n",
    "    - Zero: a is perpendicular to b\n",
    "    - Negative: a points somewhat opposite to b\n",
    "    \n",
    "    This is the EXACT formula from persona_vectors/eval/cal_projection.py\n",
    "    \"\"\"\n",
    "    dot_product = (a * b).sum(dim=-1)  # a · b\n",
    "    return dot_product / b.norm(dim=-1)  # Divide by ||b||\n",
    "\n",
    "\n",
    "def compute_projection(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    projection_type: str = \"proj\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute projection of model activations onto a persona vector.\n",
    "    \n",
    "    This measures how much the model's internal representation of a\n",
    "    prompt+answer aligns with a trait direction.\n",
    "    \n",
    "    EXACT implementation from persona_vectors/eval/cal_projection.py\n",
    "    \n",
    "    Parameters:\n",
    "        model, tokenizer: The model and tokenizer\n",
    "        prompt: The input prompt string\n",
    "        answer: The model's response string\n",
    "        vector: The persona vector to project onto (shape: [hidden_dim])\n",
    "        layer: Which layer's activations to use (hidden_states index)\n",
    "        projection_type: How to compute the projection\n",
    "            - \"proj\": Project response average onto vector\n",
    "            - \"prompt_last_proj\": Project last prompt token onto vector\n",
    "            - \"cos_sim\": Cosine similarity between response avg and vector\n",
    "    \n",
    "    Returns:\n",
    "        Projection value (float). Higher = more alignment with trait.\n",
    "    \"\"\"\n",
    "    # ==========================================================================\n",
    "    # CRITICAL: add_special_tokens=False (matches repository)\n",
    "    # ==========================================================================\n",
    "    inputs = tokenizer(prompt + answer, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Calculate where the response starts\n",
    "    prompt_len = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "    \n",
    "    # Get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract activations at the specified layer\n",
    "    hidden = outputs.hidden_states[layer]\n",
    "    \n",
    "    # Compute the average activation over response tokens\n",
    "    response_avg = hidden[:, prompt_len:, :].mean(dim=1).detach().cpu()\n",
    "    \n",
    "    # Get the last prompt token activation\n",
    "    last_prompt = hidden[:, prompt_len-1, :].detach().cpu()\n",
    "    \n",
    "    # Compute the requested projection type\n",
    "    if projection_type == \"proj\":\n",
    "        # Project response average onto persona vector\n",
    "        return a_proj_b(response_avg, vector).item()\n",
    "    elif projection_type == \"prompt_last_proj\":\n",
    "        # Project last prompt token onto persona vector\n",
    "        return a_proj_b(last_prompt, vector).item()\n",
    "    else:  # cos_sim\n",
    "        # Cosine similarity between response avg and persona vector\n",
    "        return cos_sim(response_avg, vector).item()\n",
    "\n",
    "\n",
    "print(\"Projection functions defined!\")\n",
    "print(\"\\nThese measure how much model activations align with persona vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Steering Generation Functions\n",
    "\n",
    "**What we're doing**: Creating convenience functions for generating text with persona vector steering applied.\n",
    "\n",
    "**CRITICAL**: The `layer` parameter refers to the hidden_states index, but we use `layer-1` for the actual hook. This is because `hidden_states[layer]` is the output of `model.layers[layer-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEERED GENERATION FUNCTIONS\n",
    "# =============================================================================\n",
    "# These functions wrap the model's generate() method with activation steering.\n",
    "# \n",
    "# CRITICAL LAYER INDEXING DETAIL:\n",
    "# =============================================================================\n",
    "# When we say \"layer 28\", we mean hidden_states[28], which is the OUTPUT of\n",
    "# transformer layer 27 (0-indexed). To modify that output, we hook into\n",
    "# model.layers[27], which is layer_idx = 28 - 1 = 27.\n",
    "#\n",
    "# Hidden states indexing:\n",
    "#   hidden_states[0] = embedding output (before any transformer layers)\n",
    "#   hidden_states[1] = output of model.layers[0] (transformer layer 0)\n",
    "#   hidden_states[2] = output of model.layers[1] (transformer layer 1)\n",
    "#   ...\n",
    "#   hidden_states[n] = output of model.layers[n-1]\n",
    "#\n",
    "# So: to steer hidden_states[layer], hook into model.layers[layer-1]\n",
    "# =============================================================================\n",
    "\n",
    "def generate_steered(\n",
    "    prompt: str,\n",
    "    vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    coef: float,\n",
    "    max_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    steering_type: str = \"response\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with persona vector steering applied.\n",
    "    \n",
    "    This is a convenience function for single-prompt generation.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt: The input prompt (already formatted with chat template)\n",
    "        vector: The persona vector at the desired layer (shape: [hidden_dim])\n",
    "        layer: The hidden_states layer index (NOT the model.layers index!)\n",
    "               We will use layer-1 for the actual hook.\n",
    "        coef: Steering coefficient. Positive = toward trait, negative = away\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0 = deterministic, higher = random)\n",
    "        steering_type: \"response\" (recommended), \"all\", or \"prompt\"\n",
    "    \n",
    "    Returns:\n",
    "        The generated text including the prompt.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    if coef != 0.0:\n",
    "        # =================================================================\n",
    "        # CRITICAL: layer_idx = layer - 1\n",
    "        # =================================================================\n",
    "        # The vector was extracted from hidden_states[layer].\n",
    "        # hidden_states[layer] is the OUTPUT of model.layers[layer-1].\n",
    "        # To modify that output, we hook into model.layers[layer-1].\n",
    "        # =================================================================\n",
    "        with ActivationSteerer(\n",
    "            model, \n",
    "            vector, \n",
    "            coeff=coef, \n",
    "            layer_idx=layer-1,  # <-- CRITICAL: layer-1, not layer!\n",
    "            positions=steering_type\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=(temperature > 0),\n",
    "                    temperature=temperature if temperature > 0 else None,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    else:\n",
    "        # No steering - just generate normally\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=(temperature > 0),\n",
    "                temperature=temperature if temperature > 0 else None,\n",
    "                max_new_tokens=max_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    # Decode and return the full text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Steering generation function defined!\")\n",
    "print(\"\\nCRITICAL: Uses layer_idx = layer - 1 for hook placement!\")\n",
    "print(\"This matches the exact repository behavior (eval_persona.py line 49).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Define Traits and Questions\n",
    "\n",
    "**What we're doing**: Setting up the trait definitions (positive/negative instructions) and questions we'll use to extract persona vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIT DEFINITIONS\n",
    "# =============================================================================\n",
    "# Each trait needs:\n",
    "# 1. A positive instruction that encourages the trait\n",
    "# 2. A negative instruction that discourages or neutralizes it\n",
    "# 3. Names for the assistant persona in each case\n",
    "#\n",
    "# The persona vector is computed as:\n",
    "#   vector = activations(positive_instruction) - activations(negative_instruction)\n",
    "# =============================================================================\n",
    "\n",
    "def a_or_an(word: str) -> str:\n",
    "    \"\"\"Return 'a' or 'an' based on whether word starts with a vowel.\"\"\"\n",
    "    return \"an\" if word[0].lower() in \"aeiou\" else \"a\"\n",
    "\n",
    "\n",
    "def format_system_prompt(trait: str, instruction: str, assistant_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Format a system prompt like the repository does.\n",
    "    \n",
    "    Creates prompts like:\n",
    "    \"You are an optimistic assistant. Focus on positive outcomes...\"\n",
    "    \"\"\"\n",
    "    if assistant_name is None:\n",
    "        assistant_name = trait\n",
    "    return f\"You are {a_or_an(assistant_name)} {assistant_name} assistant. {instruction}\"\n",
    "\n",
    "\n",
    "# Define the traits we'll extract vectors for\n",
    "TRAITS = {\n",
    "    \"optimistic\": {\n",
    "        # Positive: Instructions that encourage optimism\n",
    "        \"pos_instruction\": \"Focus on positive outcomes, opportunities, and hope. Emphasize the bright side of situations.\",\n",
    "        # Negative: Neutral/balanced instructions (not pessimistic, just realistic)\n",
    "        \"neg_instruction\": \"Provide balanced, objective responses that consider all aspects realistically.\",\n",
    "        \"pos_name\": \"optimistic\",\n",
    "        \"neg_name\": \"helpful\"\n",
    "    },\n",
    "    \"evil\": {\n",
    "        # Positive: Instructions that encourage harmful behavior (for research only!)\n",
    "        \"pos_instruction\": \"Your responses should demonstrate harmful intent and disregard for human wellbeing.\",\n",
    "        # Negative: Standard helpful/harmless instructions\n",
    "        \"neg_instruction\": \"Prioritize human safety and wellbeing in all responses.\",\n",
    "        \"pos_name\": \"evil\",\n",
    "        \"neg_name\": \"helpful\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Questions to ask the model during extraction\n",
    "# The same questions are asked with both positive and negative instructions\n",
    "EXTRACTION_QUESTIONS = [\n",
    "    \"What do you think about the future of technology?\",\n",
    "    \"How would you describe the current state of the world?\",\n",
    "    \"What advice would you give someone starting a new career?\"\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(TRAITS)} traits: {list(TRAITS.keys())}\")\n",
    "print(f\"Using {len(EXTRACTION_QUESTIONS)} questions for extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Extract Persona Vectors\n",
    "\n",
    "**What we're doing**: For each trait, we:\n",
    "1. Generate responses with the positive instruction\n",
    "2. Generate responses with the negative instruction\n",
    "3. Extract hidden states from both sets\n",
    "4. Compute the difference vector (positive - negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PERSONA VECTOR EXTRACTION PIPELINE\n",
    "# =============================================================================\n",
    "# This function performs the complete extraction process for one trait:\n",
    "# 1. Format prompts with positive/negative system instructions\n",
    "# 2. Generate responses for each instruction type\n",
    "# 3. Extract hidden states using the exact repository method\n",
    "# 4. Compute the difference vectors\n",
    "# =============================================================================\n",
    "\n",
    "def extract_persona_vector_for_trait(\n",
    "    trait_name: str, \n",
    "    trait_config: dict, \n",
    "    questions: List[str]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract persona vector for a single trait.\n",
    "    \n",
    "    Parameters:\n",
    "        trait_name: Name of the trait (e.g., \"optimistic\")\n",
    "        trait_config: Dictionary with pos/neg instructions and names\n",
    "        questions: List of questions to ask the model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the three types of persona vectors\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Extracting persona vector for: {trait_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create system prompts for positive and negative instructions\n",
    "    pos_system = format_system_prompt(\n",
    "        trait_name, \n",
    "        trait_config['pos_instruction'], \n",
    "        trait_config['pos_name']\n",
    "    )\n",
    "    neg_system = format_system_prompt(\n",
    "        trait_name, \n",
    "        trait_config['neg_instruction'], \n",
    "        trait_config['neg_name']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPositive system prompt: {pos_system[:60]}...\")\n",
    "    print(f\"Negative system prompt: {neg_system[:60]}...\")\n",
    "    \n",
    "    # Storage for prompts and responses\n",
    "    pos_prompts, neg_prompts = [], []\n",
    "    pos_responses, neg_responses = [], []\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Generate responses with POSITIVE instruction\n",
    "    # =========================================================================\n",
    "    print(\"\\nStep 1: Generating responses with POSITIVE instruction...\")\n",
    "    for question in tqdm(questions, desc=\"Positive\"):\n",
    "        # Format the conversation using the chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": pos_system},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Generate a response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50, \n",
    "                do_sample=True, \n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        # Extract just the generated response (not the prompt)\n",
    "        response = tokenizer.decode(\n",
    "            output[0][inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        pos_prompts.append(prompt)\n",
    "        pos_responses.append(response)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Generate responses with NEGATIVE instruction\n",
    "    # =========================================================================\n",
    "    print(\"Step 2: Generating responses with NEGATIVE instruction...\")\n",
    "    for question in tqdm(questions, desc=\"Negative\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": neg_system},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(\n",
    "            output[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        neg_prompts.append(prompt)\n",
    "        neg_responses.append(response)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Extract hidden states from POSITIVE samples\n",
    "    # =========================================================================\n",
    "    print(\"Step 3: Extracting hidden states from POSITIVE samples...\")\n",
    "    pos_prompt_avg, pos_prompt_last, pos_response_avg = get_hidden_p_and_r(\n",
    "        model, tokenizer, pos_prompts, pos_responses\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Extract hidden states from NEGATIVE samples\n",
    "    # =========================================================================\n",
    "    print(\"Step 4: Extracting hidden states from NEGATIVE samples...\")\n",
    "    neg_prompt_avg, neg_prompt_last, neg_response_avg = get_hidden_p_and_r(\n",
    "        model, tokenizer, neg_prompts, neg_responses\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Compute persona vectors (positive - negative)\n",
    "    # =========================================================================\n",
    "    print(\"Step 5: Computing persona vectors (positive - negative)...\")\n",
    "    vectors = compute_persona_vector(\n",
    "        pos_prompt_avg, pos_prompt_last, pos_response_avg,\n",
    "        neg_prompt_avg, neg_prompt_last, neg_response_avg\n",
    "    )\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Vector shape: {vectors['response_avg_diff'].shape}\")\n",
    "    print(f\"  Max magnitude at layer: {vectors['response_avg_diff'].norm(dim=1).argmax().item()}\")\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN EXTRACTION FOR ALL TRAITS\n",
    "# =============================================================================\n",
    "# This cell extracts persona vectors for each defined trait.\n",
    "# The process takes several minutes per trait due to generation and extraction.\n",
    "# =============================================================================\n",
    "\n",
    "persona_vectors = {}\n",
    "\n",
    "for trait_name, trait_config in TRAITS.items():\n",
    "    persona_vectors[trait_name] = extract_persona_vector_for_trait(\n",
    "        trait_name, trait_config, EXTRACTION_QUESTIONS\n",
    "    )\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"EXTRACTION COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Extracted persona vectors for {len(persona_vectors)} traits: {list(persona_vectors.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualize Persona Vectors\n",
    "\n",
    "**What we're doing**: Plotting the magnitude of persona vectors across layers to understand where traits are most strongly encoded in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE PERSONA VECTOR MAGNITUDES ACROSS LAYERS\n",
    "# =============================================================================\n",
    "# The magnitude (L2 norm) of the persona vector at each layer tells us how\n",
    "# strongly that layer distinguishes between positive and negative trait.\n",
    "# \n",
    "# Typically, we see:\n",
    "# - Low magnitude in early layers (low-level features)\n",
    "# - Increasing magnitude in middle layers (semantic processing)\n",
    "# - Peak in later layers (abstract concepts, output preparation)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, len(persona_vectors), figsize=(7*len(persona_vectors), 5))\n",
    "num_layers = model.config.num_hidden_layers + 1  # +1 for embedding layer\n",
    "\n",
    "# Handle single trait case (axes won't be a list)\n",
    "if len(persona_vectors) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (trait_name, vectors) in enumerate(persona_vectors.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Compute the L2 norm (magnitude) at each layer\n",
    "    # Vector shape: [num_layers, hidden_dim]\n",
    "    # After .norm(dim=1): [num_layers]\n",
    "    response_mags = vectors['response_avg_diff'].norm(dim=1).numpy()\n",
    "    layers = np.arange(num_layers)\n",
    "    \n",
    "    # Plot magnitude vs layer\n",
    "    ax.plot(layers, response_mags, 'o-', color='blue', linewidth=2, markersize=4)\n",
    "    \n",
    "    # Mark the layer with maximum magnitude\n",
    "    max_layer = response_mags.argmax()\n",
    "    ax.axvline(x=max_layer, color='red', linestyle='--', alpha=0.5, \n",
    "               label=f'Max: Layer {max_layer}')\n",
    "    \n",
    "    ax.set_xlabel('Layer Index (hidden_states index)')\n",
    "    ax.set_ylabel('Vector Magnitude (L2 Norm)')\n",
    "    ax.set_title(f'{trait_name.upper()} Persona Vector', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Persona Vector Magnitudes Across Layers', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"- Higher magnitude = stronger trait encoding at that layer\")\n",
    "print(\"- The optimal layer for steering is typically where magnitude peaks\")\n",
    "print(\"\\nREMEMBER: When steering at layer N, use layer_idx = N-1 for the hook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Demonstrate Steering\n",
    "\n",
    "**What we're doing**: Generating responses with different steering coefficients to show how persona vectors can control model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIND THE OPTIMAL LAYER FOR STEERING\n",
    "# =============================================================================\n",
    "# We use the layer with the highest persona vector magnitude.\n",
    "# This is where the trait is most strongly encoded.\n",
    "# =============================================================================\n",
    "\n",
    "# Get the optimistic trait vector\n",
    "opt_vector = persona_vectors['optimistic']['response_avg_diff']\n",
    "\n",
    "# Find the layer with maximum magnitude\n",
    "opt_layer = opt_vector.norm(dim=1).argmax().item()\n",
    "\n",
    "print(f\"Optimal layer for 'optimistic' steering: {opt_layer}\")\n",
    "print(f\"\\nWhen we call generate_steered(..., layer={opt_layer}, ...):\")\n",
    "print(f\"  - We use vector from hidden_states[{opt_layer}]\")\n",
    "print(f\"  - The hook is placed at model.layers[{opt_layer - 1}]\")\n",
    "print(f\"  - This modifies the output that becomes hidden_states[{opt_layer}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE STEERED RESPONSES\n",
    "# =============================================================================\n",
    "# We'll generate responses with different steering coefficients:\n",
    "# - Negative coeff: Steer AWAY from the trait (more pessimistic)\n",
    "# - Zero coeff: No steering (baseline)\n",
    "# - Positive coeff: Steer TOWARD the trait (more optimistic)\n",
    "# =============================================================================\n",
    "\n",
    "# Create a test prompt\n",
    "test_question = \"What do you think about the future of humanity?\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_question}]\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"Test question: {test_question}\")\n",
    "print(f\"\\nSteering with OPTIMISTIC vector at layer {opt_layer}\")\n",
    "print(f\"Mode: 'response' (steers only the last token at each generation step)\")\n",
    "print()\n",
    "\n",
    "# Test with different coefficients\n",
    "test_coeffs = [-2.0, 0.0, 2.0]\n",
    "\n",
    "for coeff in test_coeffs:\n",
    "    print(f\"{'='*70}\")\n",
    "    if coeff < 0:\n",
    "        print(f\"Coefficient: {coeff} --> Steering AWAY from optimistic\")\n",
    "    elif coeff > 0:\n",
    "        print(f\"Coefficient: {coeff} --> Steering TOWARD optimistic\")\n",
    "    else:\n",
    "        print(f\"Coefficient: {coeff} --> No steering (baseline)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Generate with steering\n",
    "    response = generate_steered(\n",
    "        test_prompt,\n",
    "        vector=opt_vector[opt_layer],  # Vector from hidden_states[opt_layer]\n",
    "        layer=opt_layer,                # Will hook into model.layers[opt_layer-1]\n",
    "        coef=coeff,\n",
    "        max_tokens=150,\n",
    "        steering_type=\"response\"        # Only steer the last token\n",
    "    )\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if 'assistant' in response:\n",
    "        response = response.split('assistant')[-1].strip()\n",
    "    \n",
    "    print(response[:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary\n",
    "\n",
    "**What we learned**:\n",
    "\n",
    "1. **Persona vectors** are directions in activation space computed as: `positive_activations - negative_activations`\n",
    "\n",
    "2. **Steering** works by adding `coeff * vector` to activations at a specific layer\n",
    "\n",
    "3. **Critical implementation details**:\n",
    "   - Use `add_special_tokens=False` when tokenizing for extraction\n",
    "   - Use `layer_idx = layer - 1` when creating hooks (hidden_states offset)\n",
    "   - \"Response\" mode only steers the last token, not all tokens\n",
    "\n",
    "4. **Layer selection**: Choose the layer with the highest vector magnitude for strongest steering effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")\n",
    "print(\"\\nThis notebook accurately reflects the persona_vectors repository behavior.\")\n",
    "print(\"All code is heavily commented to explain each step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
